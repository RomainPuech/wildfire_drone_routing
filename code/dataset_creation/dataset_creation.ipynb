{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from rasterio.warp import transform_bounds\n",
    "from shapely.geometry import Polygon\n",
    "from pyproj import Transformer\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(\"../..\") + \"/code\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataset import load_scenario\n",
    "from Scenario_sampler import ScenarioSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "def return_first_scenario(path):\n",
    "    chosen_subfolder = None\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_dir() and entry.name != '.DS_Store':\n",
    "                chosen_subfolder = entry.name\n",
    "                break\n",
    "    if chosen_subfolder is None:\n",
    "        raise ValueError(\"No subfolder found\")\n",
    "    return chosen_subfolder\n",
    "\n",
    "\n",
    "\n",
    "def read_first_and_last_lines(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Read the first line\n",
    "        first_line = file.readline().decode()\n",
    "\n",
    "        # Move to the end of the file\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        file_size = file.tell()\n",
    "        if file_size == 0:\n",
    "            return '', ''  # Empty file\n",
    "\n",
    "        pos = file_size - 1\n",
    "\n",
    "        # Skip trailing newlines at the end of the file\n",
    "        while pos >= 0:\n",
    "            file.seek(pos, os.SEEK_SET)\n",
    "            char = file.read(1)\n",
    "            if char != b'\\n' and char != b'\\r':\n",
    "                break\n",
    "            pos -= 1\n",
    "\n",
    "        # Now find the previous newline (start of last line)\n",
    "        while pos >= 0:\n",
    "            file.seek(pos, os.SEEK_SET)\n",
    "            if file.read(1) == b'\\n':\n",
    "                pos += 1\n",
    "                break\n",
    "            pos -= 1\n",
    "        else:\n",
    "            pos = 0  # The last line is also the first line\n",
    "\n",
    "        file.seek(pos, os.SEEK_SET)\n",
    "        last_line = file.readline().decode()\n",
    "\n",
    "    return first_line, last_line\n",
    "\n",
    "\n",
    "def read_first_and_last_lines2(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the first line\n",
    "        first_line = file.readline()\n",
    "\n",
    "        # Move to the end of the file\n",
    "        file.seek(-1, 2)  # 2 means seek from the end\n",
    "\n",
    "        # Read until a newline is found\n",
    "        last_line = ''\n",
    "        while True:\n",
    "            char = file.read(1)\n",
    "            if char == '\\n' or file.tell() == 1:\n",
    "                break\n",
    "            last_line = char + last_line\n",
    "            file.seek(-2, 1)  # Move back two characters\n",
    "\n",
    "    return first_line, last_line\n",
    "\n",
    "from datetime import datetime\n",
    "def find_earliest_latest_dates(layout_path):\n",
    "    \"\"\"\n",
    "    Find the earliest and latest dates in a layout folder.\n",
    "    \"\"\"\n",
    "    earliest_date = None\n",
    "    latest_date = None\n",
    "    for filename in os.listdir(os.path.join(layout_path, \"Weather_Data\")):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            first_line, last_line = read_first_and_last_lines(os.path.join(layout_path, \"Weather_Data\", filename))\n",
    "            first_date = \" \".join(first_line.split(\" \")[:4])\n",
    "            last_date = \" \".join(last_line.split(\" \")[:4])\n",
    "            first_date = datetime.strptime(first_date, \"%Y %m %d %H%M\")\n",
    "            last_date = datetime.strptime(last_date, \"%Y %m %d %H%M\")\n",
    "            if earliest_date is None or first_date < earliest_date:\n",
    "                earliest_date = first_date\n",
    "            if latest_date is None or last_date > latest_date:\n",
    "                latest_date = last_date\n",
    "    return earliest_date, latest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/wildfire/lib/python3.11/site-packages/pyogrio/geopandas.py:265: UserWarning: More than one layer found in 'FPA_FOD_20221014.gpkg': 'Fires' (default), 'NWCG_UnitIdActive_20200123'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2303566\n",
      "2885857\n",
      "2843606\n"
     ]
    }
   ],
   "source": [
    "new_fires_gdf = gpd.read_file(\"./newfires.gpkg\") # 6th edition # gpd.read_file(\"./FPA_FOD_20210617.gpkg\") 5th edition  \n",
    "# replace illegal dates by jan first 1900\n",
    "new_fires_gdf['DISCOVERYDATETIME'] = new_fires_gdf['DISCOVERYDATETIME'].replace('1001/01/01 00:00:00+00', '1900/01/01 00:00:00+00')\n",
    "new_fires_gdf['DISCOVERY_DATE'] = pd.to_datetime(new_fires_gdf['DISCOVERYDATETIME'])\n",
    "new_fires_gdf['DISCOVERY_DATE'].dt.date\n",
    "new_fires_gdf['LONGITUDE'] = new_fires_gdf['LONGDD83']\n",
    "new_fires_gdf['LATITUDE'] = new_fires_gdf['LATDD83']\n",
    "new_fires_gdf = new_fires_gdf.to_crs(\"EPSG:4326\")\n",
    "print(len(new_fires_gdf)) # \n",
    "\n",
    "old_fires_gdf = gpd.read_file(\"./FPA_FOD_20221014.gpkg\")\n",
    "old_fires_gdf['OBJECTID'] = old_fires_gdf['FOD_ID']\n",
    "old_fires_gdf = old_fires_gdf.to_crs(\"EPSG:4326\")\n",
    "print(len(old_fires_gdf)) # 2 303 566\n",
    "\n",
    "# merge the two gdfs\n",
    "fires_gdf = pd.concat([new_fires_gdf, old_fires_gdf])\n",
    "print(len(fires_gdf))\n",
    "\n",
    "# drop duplicates as defined as same lat, long and same date\n",
    "fires_gdf = fires_gdf.drop_duplicates(subset=['LATITUDE', 'LONGITUDE', 'DISCOVERY_DATE'])\n",
    "print(len(fires_gdf)) \n",
    "\n",
    "# fires_gdf = new_fires_gdf.to_crs(\"EPSG:4326\")\n",
    "# print(len(fires_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07-03'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_fires_gdf.iloc[0]['DISCOVERY_DATE'].strftime('%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './WideDataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m tif_path = \u001b[33m\"\u001b[39m\u001b[33msim2real_layouts\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m os.makedirs(tif_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os.listdir(dataset_path):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m folder == \u001b[33m\"\u001b[39m\u001b[33m.DS_Store\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './WideDataset/'"
     ]
    }
   ],
   "source": [
    "# Get the layout coordinates \n",
    "# 1 extract the tif files\n",
    "dataset_path = \"./WideDataset/\"\n",
    "\n",
    "# copy the layout tifs from a folder to a folder called sim2real_layout\n",
    "tif_path = \"sim2real_layouts\"\n",
    "os.makedirs(tif_path, exist_ok=True)\n",
    "for folder in os.listdir(dataset_path):\n",
    "    if folder == \".DS_Store\":\n",
    "        continue\n",
    "    shutil.copy(f\"WideDataset/{folder}/Vegetation_Map/Existing_Vegetation_Cover.tif\", f\"{tif_path}/{folder}_Existing_Vegetation_Cover.tif\")\n",
    "\n",
    "print(\"Copied the layout tifs to sim2real_layouts\")\n",
    "####\n",
    "\n",
    "tif_files = glob(os.path.join(tif_path, \"*.tif\"))\n",
    "layout_list = []\n",
    "widths = []\n",
    "names = []\n",
    "for tif_file in tif_files:\n",
    "    with rasterio.open(tif_file) as dataset:\n",
    "        # get the file name without the path\n",
    "        file_name = os.path.basename(tif_file)  # '0004_Elevation.tif'\n",
    "        identifier = \"_\".join(file_name.split('_')[:2])\n",
    "\n",
    "        # extract the resolution, check it is 30\n",
    "        x_resolution = dataset.transform[0]\n",
    "        y_resolution = -dataset.transform[4]\n",
    "        assert x_resolution == y_resolution == 30, f\"Resolution is not the same: {x_resolution} != {y_resolution}\"\n",
    "\n",
    "        # extract the coordinates using the bounds\n",
    "        # /!\\ DO NOT USE transform_bounds(dataset.crs, 'EPSG:4326', *dataset.bounds)\n",
    "        x_min = dataset.bounds[0]\n",
    "        x_max = dataset.bounds[2]\n",
    "        y_min = dataset.bounds[1]\n",
    "        y_max = dataset.bounds[3]\n",
    "\n",
    "        transformer = Transformer.from_crs(dataset.crs, \"EPSG:4326\", always_xy=True)\n",
    "        lat_top_left, lon_top_left = transformer.transform(x_min, y_max)\n",
    "        lat_top_right, lon_top_right = transformer.transform(x_max, y_max)\n",
    "        lat_bottom_left, lon_bottom_left = transformer.transform(x_min, y_min)\n",
    "        lat_bottom_right, lon_bottom_right = transformer.transform(x_max, y_min)\n",
    "\n",
    "        # Create the polygon using the transformed bounds\n",
    "        polygon = Polygon((\n",
    "            (lat_top_left, lon_top_left),\n",
    "            (lat_top_right, lon_top_right),\n",
    "            (lat_bottom_right, lon_bottom_right),\n",
    "            (lat_bottom_left, lon_bottom_left),\n",
    "            (lat_top_left, lon_top_left)  # close the polygon\n",
    "        ))\n",
    "\n",
    "        layout_list.append({\n",
    "            'identifier': identifier,\n",
    "            'height': dataset.height,\n",
    "            'width': dataset.width,\n",
    "            'geometry': polygon,\n",
    "            'transformer': transformer,\n",
    "            'dataset': dataset\n",
    "        })\n",
    "\n",
    "\n",
    "        widths.append(dataset.width)\n",
    "        names.append(identifier)\n",
    "\n",
    "sorted_indices = np.argsort(widths)\n",
    "widths = np.array(widths)[sorted_indices]\n",
    "names = np.array(names)[sorted_indices]\n",
    "filtered_layout_list = [layout_list[i] for i in sorted_indices]\n",
    "\n",
    "n_small_layouts = len(widths[widths < 500])\n",
    "n_medium_layouts = len(widths[(widths >= 500) & (widths < 1000)])\n",
    "n_large_layouts = len(widths[widths >= 1000])\n",
    "\n",
    "small_layouts = filtered_layout_list[:n_small_layouts]\n",
    "medium_layouts = filtered_layout_list[n_small_layouts:n_small_layouts + n_medium_layouts]\n",
    "large_layouts = filtered_layout_list[n_small_layouts + n_medium_layouts:]\n",
    "\n",
    "selected_layouts = medium_layouts + large_layouts\n",
    "widths = widths[-len(selected_layouts):]\n",
    "names = names[-len(selected_layouts):]\n",
    "print(max(widths))\n",
    "raise Exception(\"stop\")\n",
    "\n",
    "\n",
    "print(\"loaded the layout list\")\n",
    "# move the samll layouts to the small folder\n",
    "os.makedirs(\"small_layouts\", exist_ok=True)\n",
    "for layout in small_layouts:\n",
    "    #print(os.path.join(dataset_path, layout['identifier']), os.path.join(\"small_layouts\", layout['identifier']))\n",
    "    try:\n",
    "        shutil.move(os.path.join(dataset_path, layout['identifier']), os.path.join(\"small_layouts\", layout['identifier']))\n",
    "    except Exception as e:\n",
    "       # file already moved\n",
    "       pass\n",
    "\n",
    "\n",
    "# convert the layout list to a geopandas df\n",
    "layout_list = selected_layouts\n",
    "gdf = gpd.GeoDataFrame(layout_list, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the historical fires \n",
    "# fires_gdf = gpd.read_file(\"./FPA_FOD_20210617.gpkg\")\n",
    "# fires_gdf = fires_gdf.to_crs(\"EPSG:4326\")\n",
    "print(\"loaded the fires\")\n",
    "\n",
    "\n",
    "# Joint\n",
    "# Spatial join: find which points fall into which polygons\n",
    "joined = gpd.sjoin(fires_gdf, gdf, how='inner', predicate='within')\n",
    "\n",
    "\n",
    "# Count points per polygon\n",
    "counts = joined.groupby('identifier').size().reset_index(name='fire_count')\n",
    "\n",
    "\n",
    "dataset_path = \"./WideDataset/\"\n",
    "scenario_path_suffix = \"/Satellite_Images_Mask/\"\n",
    "\n",
    "failed_layouts = []\n",
    "continue_out = False\n",
    "processed = total = 0\n",
    "\n",
    "for layout_name in tqdm.tqdm(names):\n",
    "    total+=1\n",
    "    print(layout_name)\n",
    "    try:\n",
    "        layout_folder = dataset_path + layout_name + scenario_path_suffix\n",
    "        # check the layout folder exists\n",
    "        if not os.path.exists(layout_folder):\n",
    "            print(f\"Layout {layout_folder} does not exist\")\n",
    "            layout_folder = dataset_path + layout_name +  \"/Satellite_Image_Mask/\"\n",
    "            if not os.path.exists(layout_folder):\n",
    "                print(f\"Layout {layout_folder} does not exist\")\n",
    "                failed_layouts.append(layout_name)\n",
    "                continue\n",
    "        \n",
    "        if os.path.exists(f\"./WideDataset/{layout_name}/selected_scenarios_full_match.txt\"):\n",
    "            processed +=1\n",
    "            #continue\n",
    "\n",
    "        # check that the scenario have the right size\n",
    "        first_scenario = return_first_scenario(layout_folder)\n",
    "        if first_scenario is None:\n",
    "            print(f\"Layout {layout_name} does not have any scenario\")\n",
    "            failed_layouts.append(layout_name)\n",
    "            continue\n",
    "        \n",
    "        first_loaded_scenario = load_scenario(os.path.join(layout_folder, first_scenario), extension = '.jpg', first_frame_only=True)\n",
    "        height_scenario, width_scenario = first_loaded_scenario.shape[0], first_loaded_scenario.shape[1]\n",
    "\n",
    "        data = joined[joined['identifier'] == layout_name]\n",
    "        if len(data) == 0:\n",
    "            print(f\"No fires found for layout {layout_name}\")\n",
    "            failed_layouts.append(layout_name)\n",
    "            continue\n",
    "        plt.scatter(data[\"LONGITUDE\"], data[\"LATITUDE\"])\n",
    "        plt.axis(\"equal\")\n",
    "        plt.show()\n",
    "\n",
    "        #earliest_date, latest_date = find_earliest_latest_dates(dataset_path + layout_name)\n",
    "        #print(f\"Earliest date: {earliest_date}, Latest date: {latest_date}\")\n",
    "\n",
    "\n",
    "        sampler = ScenarioSampler(layout_folder, extension = '.jpg')\n",
    "        sampled_scenarios = []\n",
    "        sampled_ignition_points = []\n",
    "        associated_fires = []\n",
    "        date_matched = []\n",
    "        distances = []\n",
    "        failed = 0\n",
    "        # plot the historical fires\n",
    "        \n",
    "\n",
    "        # start with the fires that have potewntial to be test fires, i.e their date is between the earliest and latest date\n",
    "        # We will have one \"test\" dataset, one \"train\" dataset, and one extra train dataset for the test fires\n",
    "        # filtered_data = data[\n",
    "        # (data['DISCOVERY_DATE'].dt.date >= earliest_date.date()) & \n",
    "        # (data['DISCOVERY_DATE'].dt.date <= latest_date.date())\n",
    "        # ]\n",
    "        \n",
    "\n",
    "        for i, fire in data.iterrows():\n",
    "            # print the coordinates of the fire\n",
    "            width, height = fire['width'], fire['height']\n",
    "            # check that the scenario have the right size\n",
    "            if width != width_scenario or height != height_scenario:\n",
    "                print(f\"Scenario {first_scenario} has the wrong size for layout {layout_name}: {width} != {width_scenario} or {height} != {height_scenario}\")\n",
    "                failed_layouts.append(layout_name)\n",
    "                break\n",
    "            dataset = fire['dataset']\n",
    "            transformer = fire['transformer']\n",
    "            x_fire, y_fire = transformer.transform(fire['LONGITUDE'], fire['LATITUDE'], direction='INVERSE')\n",
    "            row, col = rasterio.transform.rowcol(dataset.transform, x_fire, y_fire)\n",
    "            # print(\"row, col\", row, col)\n",
    "            ignition_point = (col, row)\n",
    "            sample, sampled_ignition_point = sampler.get_scenario_location(ignition_point, leeway_distance=5, sampling_method='closest', exclude_scenarios=sampled_scenarios)\n",
    "            if sample is None:\n",
    "                failed += 1\n",
    "                continue\n",
    "            sampled_scenarios.append(sample)\n",
    "            sampled_ignition_points.append(sampled_ignition_point)\n",
    "            associated_fires.append(fire['OBJECTID'])\n",
    "            distance = abs(sampled_ignition_point[0] - ignition_point[0]) + abs(sampled_ignition_point[1] - ignition_point[1])\n",
    "            distances.append(distance)\n",
    "        # plot the sampled scenarios\n",
    "        # the axes are inverted as coordinates start in (0,0) in the top left corner\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter([point[0] for point in sampled_ignition_points], [width - point[1] for point in sampled_ignition_points], color='red')\n",
    "        plt.show()\n",
    "        print(f\"Failed {failed}\")\n",
    "        # write the selected scenarios in a txt file\n",
    "        with open(f\"./WideDataset/{layout_name}/selected_scenarios.txt\", \"w\") as f:\n",
    "            for scenario, fire_id in zip(sampled_scenarios, associated_fires):\n",
    "                f.write(f\"{scenario}, {fire_id}\\n\")\n",
    "            f.write(f\"Failed: {failed}\\n\")\n",
    "            failed_percentage = failed / max(len(data),1)\n",
    "            f.write(f\"Failed percentage: {failed_percentage}\\n\")\n",
    "        if len(data) == 0 or failed_percentage > 0.2:\n",
    "            print(f\"!! Failed {failed} out of {len(data)} for layout {layout_name} !!\")\n",
    "            failed_layouts.append(layout_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for layout {layout_name}: {e}\")\n",
    "        failed_layouts.append(layout_name)\n",
    "print(processed)\n",
    "print(total)\n",
    "    \n",
    "\n",
    "# TODO do we need to create the grid manually? I think the raster file will doirectly give you the coordinates within the layout\n",
    "\n",
    "\n",
    "# for each fire, sample the scenario (space only, and time+space)\n",
    "# write the identifier in a txt file (one for space only, one for time+space)\n",
    "\n",
    "# move the scenarios into a selected folder\n",
    "# \"delete\" the other scenarios\n",
    "\n",
    "# train test split with the date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same with date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# # Path to your GeoJSON file\n",
    "# geojson_file = './National_USFS_Fire_Occurrence_Point_(Feature_Layer).geojson'\n",
    "\n",
    "# # Read the GeoJSON file\n",
    "# gdf = gpd.read_file(geojson_file)\n",
    "\n",
    "# # Path to the output GeoPackage file\n",
    "# gpkg_file = 'newfires.gpkg'\n",
    "\n",
    "# # Save the GeoDataFrame to a GeoPackage file\n",
    "# gdf.to_file(gpkg_file, driver='GPKG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fires_gdf = gpd.read_file(\"./newfires.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582291\n",
      "581759\n"
     ]
    }
   ],
   "source": [
    "# remove all lines with DISCOVERYDATETIME = 1001/01/01 00:00:00+00\n",
    "print(len(new_fires_gdf))\n",
    "new_fires_gdf = new_fires_gdf[new_fires_gdf['DISCOVERYDATETIME'] != '1001/01/01 00:00:00+00']\n",
    "print(len(new_fires_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2014-07-03\n",
       "1         2014-06-06\n",
       "2         2014-07-28\n",
       "3         2014-08-19\n",
       "4         2014-08-26\n",
       "             ...    \n",
       "582286    2005-09-11\n",
       "582287    2010-04-08\n",
       "582288    2009-09-29\n",
       "582289    1996-07-06\n",
       "582290    2021-07-14\n",
       "Name: DISCOVERY_DATE, Length: 581759, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_fires_gdf['DISCOVERY_DATE'] = pd.to_datetime(new_fires_gdf['DISCOVERYDATETIME'])\n",
    "new_fires_gdf['DISCOVERY_DATE'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POINT (-107.24360000079399 36.97472000088236)\n",
      "36.97471 -107.24359\n"
     ]
    }
   ],
   "source": [
    "print(new_fires_gdf['geometry'].iloc[1])\n",
    "print(new_fires_gdf['LATDD83'].iloc[1], new_fires_gdf['LONGDD83'].iloc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fires_gdf['DISCOVERY_DATE'] = pd.to_datetime(new_fires_gdf['DISCOVERYDATETIME'])\n",
    "new_fires_gdf['DISCOVERY_DATE'].dt.date\n",
    "new_fires_gdf['LONGITUDE'] = new_fires_gdf['LONGDD83']\n",
    "new_fires_gdf['LATITUDE'] = new_fires_gdf['LATDD83']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24    2019-11-01 14:30:00+00:00\n",
       "266   2019-07-25 15:37:00+00:00\n",
       "267   2019-09-06 16:46:00+00:00\n",
       "347   2022-04-20 21:56:00+00:00\n",
       "348   2022-06-12 22:24:00+00:00\n",
       "Name: DISCOVERY_DATE, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only the fires that occured after 2019\n",
    "new_fires_gdf = new_fires_gdf[new_fires_gdf['DISCOVERY_DATE'] >= '2019-01-01']\n",
    "print(len(new_fires_gdf))\n",
    "new_fires_gdf['DISCOVERY_DATE'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied the layout tifs to sim2real_layouts\n",
      "loaded the layout list\n",
      "loaded the fires\n",
      "    identifier  fire_count\n",
      "0   0016_03070         192\n",
      "1   0037_01578          21\n",
      "2   0041_02386          66\n",
      "3   0057_03186         279\n",
      "4   0058_03866         183\n",
      "5   0059_02804         264\n",
      "6   0060_03010         381\n",
      "7   0061_03726         328\n",
      "8   0062_03187          55\n",
      "9   0063_02387         896\n",
      "10  0064_02717         172\n",
      "11  0065_03061         116\n",
      "12  0066_03773         174\n",
      "13  0067_03550         280\n",
      "14  0068_04211         142\n",
      "15  0069_03539         629\n",
      "16  0081_03471         252\n",
      "17  0082_03155         691\n",
      "18  0083_02892        1098\n",
      "19  0084_02609         249\n",
      "20  0090_00987         537\n",
      "21  0092_03189          26\n",
      "22  0093_01748           1\n",
      "23  0094_01688           1\n",
      "24  0095_01726           1\n",
      "25  0098_01784         544\n",
      "26  0100_02449         544\n",
      "27  0102_01733         714\n",
      "28  0103_01810         714\n",
      "29  0104_02422         180\n",
      "30  0105_03054         142\n",
      "31  0106_02165          43\n",
      "32  0111_03612          59\n",
      "33  0113_03495        2379\n",
      "34  0114_02292        2379\n",
      "35  0243_02722          10\n",
      "36  0244_03110        1474\n",
      "37  0245_03988        2032\n",
      "38  0246_00984          25\n",
      "39  0247_03453          23\n",
      "40  0248_01962         597\n",
      "41  0249_02843          30\n",
      "42  0250_02864          36\n",
      "43  0251_02843          58\n",
      "44  0252_02927          51\n",
      "45  0253_03246         645\n",
      "46  0254_02361         645\n",
      "47  0255_02103         645\n",
      "48  0256_02752         645\n",
      "49  0257_02175        2003\n",
      "50  0258_02858        2003\n",
      "51  0259_02663         111\n",
      "52  0260_03391         147\n",
      "53  0261_02900         430\n",
      "54  0262_03319         309\n",
      "55  0270_02277         213\n",
      "56  0324_02925         264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/72 [00:00<00:01, 35.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0344_03155: No subfolder found\n",
      "Layout ./WideDataset/0106_02165/Satellite_Images_Mask/ does not exist\n",
      "Layout ./WideDataset/0111_03612/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 9/72 [00:00<00:02, 22.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout ./WideDataset/0104_02422/Satellite_Images_Mask/ does not exist\n",
      "Error for layout 0089_00984: No subfolder found\n",
      "we wanted to sample the fire 2023-07-24 23:34:00+00:00 at (396, 62) but it failed\n",
      "we wanted to sample the fire 2023-07-24 23:34:00+00:00 at (396, 62) but it failed\n",
      "!! Failed 2 out of 2 for layout 0016_03070 !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12/72 [00:02<00:14,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout ./WideDataset/0105_03054/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 14/72 [00:02<00:11,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0259_02663: unconverted data remains:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 22/72 [00:02<00:05,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout ./WideDataset/0243_02722/Satellite_Images_Mask/ does not exist\n",
      "Error for layout 0013_01466: No subfolder found\n",
      "Error for layout 0012_02094: No subfolder found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 32/72 [00:03<00:04,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0065_03061: unconverted data remains:  0\n",
      "Layout ./WideDataset/0064_02717/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 34/72 [00:04<00:05,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout ./WideDataset/0059_02804/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 35/72 [00:04<00:05,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0103_01810: No subfolder found\n",
      "Error for layout 0102_01733: No subfolder found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 41/72 [00:05<00:04,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0261_02900: unconverted data remains:  \n",
      "Error for layout 0057_03186: unconverted data remains:  \n",
      "Error for layout 0090_00987: No subfolder found\n",
      "Layout ./WideDataset/0242_02940/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 45/72 [00:05<00:02, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0245_03988: No JPG files found in folder: ./WideDataset/0245_03988/Satellite_Images_Mask/0245_03900/\n",
      "Error for layout 0081_03471: unconverted data remains:  0\n",
      "Layout ./WideDataset/0060_03010/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 47/72 [00:05<00:02,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0060_03010: unconverted data remains:  \n",
      "Layout ./WideDataset/0062_03187/Satellite_Images_Mask/ does not exist\n",
      "Error for layout 0062_03187: unconverted data remains:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 49/72 [00:05<00:02,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0262_03319: unconverted data remains:  \n",
      "Layout ./WideDataset/0058_03866/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 50/72 [00:06<00:02,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout ./WideDataset/0244_03110/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 51/72 [00:06<00:03,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0244_03110: unconverted data remains:  \n",
      "Error for layout 0093_01748: No subfolder found\n",
      "Error for layout 0095_01726: No subfolder found\n",
      "Error for layout 0094_01688: No subfolder found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 55/72 [00:06<00:02,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0083_02892: unconverted data remains:  \n",
      "Error for layout 0021_01232: No subfolder found\n",
      "Error for layout 0020_00970: No subfolder found\n",
      "Layout ./WideDataset/0063_02387/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 58/72 [00:07<00:01,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0063_02387: unconverted data remains:  \n",
      "Layout ./WideDataset/0061_03726/Satellite_Images_Mask/ does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:07<00:00,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for layout 0098_01784: No subfolder found\n",
      "Error for layout 0100_02449: No subfolder found\n",
      "Error for layout 0113_03495: No subfolder found\n",
      "Error for layout 0114_02292: No subfolder found\n",
      "Error for layout 0257_02175: No subfolder found\n",
      "Error for layout 0258_02858: No subfolder found\n",
      "Error for layout 0255_02103: No subfolder found\n",
      "Error for layout 0254_02361: No subfolder found\n",
      "Error for layout 0256_02752: No subfolder found\n",
      "Error for layout 0253_03246: No subfolder found\n",
      "71\n",
      "72\n",
      "['0016_03070']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Scenario_sampler import ScenarioSamplerDate\n",
    "from tqdm import tqdm\n",
    "# Get the layout coordinates \n",
    "# 1 extract the tif files\n",
    "dataset_path = \"./WideDataset/\"\n",
    "\n",
    "# copy the layout tifs from a folder to a folder called sim2real_layout\n",
    "tif_path = \"sim2real_layouts\"\n",
    "os.makedirs(tif_path, exist_ok=True)\n",
    "for folder in os.listdir(dataset_path):\n",
    "    if folder == \".DS_Store\":\n",
    "        continue\n",
    "    shutil.copy(f\"WideDataset/{folder}/Vegetation_Map/Existing_Vegetation_Cover.tif\", f\"{tif_path}/{folder}_Existing_Vegetation_Cover.tif\")\n",
    "\n",
    "print(\"Copied the layout tifs to sim2real_layouts\")\n",
    "####\n",
    "\n",
    "tif_files = glob(os.path.join(tif_path, \"*.tif\"))\n",
    "layout_list = []\n",
    "widths = []\n",
    "names = []\n",
    "for tif_file in tif_files:\n",
    "    with rasterio.open(tif_file) as dataset:\n",
    "        # get the file name without the path\n",
    "        file_name = os.path.basename(tif_file)  # '0004_Elevation.tif'\n",
    "        identifier = \"_\".join(file_name.split('_')[:2])\n",
    "\n",
    "        # extract the resolution, check it is 30\n",
    "        x_resolution = dataset.transform[0]\n",
    "        y_resolution = -dataset.transform[4]\n",
    "        assert x_resolution == y_resolution == 30, f\"Resolution is not the same: {x_resolution} != {y_resolution}\"\n",
    "\n",
    "        # extract the coordinates using the bounds\n",
    "        # /!\\ DO NOT USE transform_bounds(dataset.crs, 'EPSG:4326', *dataset.bounds)\n",
    "        x_min = dataset.bounds[0]\n",
    "        x_max = dataset.bounds[2]\n",
    "        y_min = dataset.bounds[1]\n",
    "        y_max = dataset.bounds[3]\n",
    "\n",
    "        transformer = Transformer.from_crs(dataset.crs, \"EPSG:4326\", always_xy=True)\n",
    "        lat_top_left, lon_top_left = transformer.transform(x_min, y_max)\n",
    "        lat_top_right, lon_top_right = transformer.transform(x_max, y_max)\n",
    "        lat_bottom_left, lon_bottom_left = transformer.transform(x_min, y_min)\n",
    "        lat_bottom_right, lon_bottom_right = transformer.transform(x_max, y_min)\n",
    "\n",
    "        # Create the polygon using the transformed bounds\n",
    "        polygon = Polygon((\n",
    "            (lat_top_left, lon_top_left),\n",
    "            (lat_top_right, lon_top_right),\n",
    "            (lat_bottom_right, lon_bottom_right),\n",
    "            (lat_bottom_left, lon_bottom_left),\n",
    "            (lat_top_left, lon_top_left)  # close the polygon\n",
    "        ))\n",
    "\n",
    "        layout_list.append({\n",
    "            'identifier': identifier,\n",
    "            'height': dataset.height,\n",
    "            'width': dataset.width,\n",
    "            'geometry': polygon,\n",
    "            'transformer': transformer,\n",
    "            'dataset': dataset\n",
    "        })\n",
    "\n",
    "\n",
    "        widths.append(dataset.width)\n",
    "        names.append(identifier)\n",
    "\n",
    "sorted_indices = np.argsort(widths)\n",
    "widths = np.array(widths)[sorted_indices]\n",
    "names = np.array(names)[sorted_indices]\n",
    "filtered_layout_list = [layout_list[i] for i in sorted_indices]\n",
    "\n",
    "n_small_layouts = len(widths[widths < 500])\n",
    "n_medium_layouts = len(widths[(widths >= 500) & (widths < 1000)])\n",
    "n_large_layouts = len(widths[widths >= 1000])\n",
    "\n",
    "small_layouts = filtered_layout_list[:n_small_layouts]\n",
    "medium_layouts = filtered_layout_list[n_small_layouts:n_small_layouts + n_medium_layouts]\n",
    "large_layouts = filtered_layout_list[n_small_layouts + n_medium_layouts:]\n",
    "\n",
    "print(\"loaded the layout list\")\n",
    "\n",
    "\n",
    "# convert the layout list to a geopandas df\n",
    "gdf = gpd.GeoDataFrame(layout_list, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the historical fires \n",
    "# fires_gdf = gpd.read_file(\"./FPA_FOD_20210617.gpkg\")\n",
    "# fires_gdf = fires_gdf.to_crs(\"EPSG:4326\")\n",
    "print(\"loaded the fires\")\n",
    "\n",
    "\n",
    "# Joint\n",
    "# Spatial join: find which points fall into which polygons\n",
    "joined = gpd.sjoin(new_fires_gdf, gdf, how='inner', predicate='within')\n",
    "\n",
    "\n",
    "# Count points per polygon\n",
    "counts = joined.groupby('identifier').size().reset_index(name='fire_count')\n",
    "print(counts)\n",
    "\n",
    "\n",
    "dataset_path = \"./WideDataset/\"\n",
    "scenario_path_suffix = \"/Satellite_Images_Mask/\"\n",
    "\n",
    "failed_layouts = []\n",
    "processed_layouts = []\n",
    "continue_out = False\n",
    "processed = total = 0\n",
    "WORKED = 0\n",
    "\n",
    "for layout_name in tqdm(names):\n",
    "    total+=1\n",
    "    #print(layout_name)\n",
    "    try:\n",
    "        layout_folder = dataset_path + layout_name + scenario_path_suffix\n",
    "        # check the layout folder exists\n",
    "        if not os.path.exists(layout_folder):\n",
    "            print(f\"Layout {layout_folder} does not exist\")\n",
    "            layout_folder = dataset_path + layout_name +  \"/Satellite_Image_Mask/\"\n",
    "            if not os.path.exists(layout_folder):\n",
    "                print(f\"Layout {layout_folder} does not exist\")\n",
    "                failed_layouts.append(layout_name)\n",
    "                continue\n",
    "        \n",
    "        if os.path.exists(f\"./WideDataset/{layout_name}/selected_scenarios.txt\"):\n",
    "            processed +=1\n",
    "            #continue\n",
    "\n",
    "        # check that the scenario have the right size\n",
    "        first_scenario = return_first_scenario(layout_folder)\n",
    "        if first_scenario is None:\n",
    "            print(f\"Layout {layout_name} does not have any scenario\")\n",
    "            failed_layouts.append(layout_name)\n",
    "            continue\n",
    "        \n",
    "        first_loaded_scenario = load_scenario(os.path.join(layout_folder, first_scenario), extension = '.jpg', first_frame_only=True)\n",
    "        height_scenario, width_scenario = first_loaded_scenario.shape[0], first_loaded_scenario.shape[1]\n",
    "\n",
    "        earliest_date, latest_date = find_earliest_latest_dates(dataset_path + layout_name)\n",
    "\n",
    "        # if earliest_date.year > 2020:\n",
    "        #     print(f\"Layout {layout_name} has no fires before 2021: {earliest_date}\")\n",
    "        #     failed_layouts.append(layout_name)\n",
    "        #     continue\n",
    "\n",
    "        # plot the historical fires\n",
    "        data = joined[joined['identifier'] == layout_name]\n",
    "\n",
    "        filtered_data = data[\n",
    "        (data['DISCOVERY_DATE'].dt.date >= earliest_date.date()) & \n",
    "        (data['DISCOVERY_DATE'].dt.date <= latest_date.date())\n",
    "        ]\n",
    "        n_data = len(filtered_data)\n",
    "        if n_data == 0:\n",
    "            #print(f\"No fires found for layout {layout_name} between {earliest_date} and {latest_date}\")\n",
    "            # if len(data) != 0:  \n",
    "            #     print(\"example  fire date:\", data['DISCOVERY_DATE'].iloc[0])\n",
    "            failed_layouts.append(layout_name)\n",
    "            continue\n",
    "\n",
    "\n",
    "        sampler = ScenarioSamplerDate(layout_folder, extension = '.jpg')\n",
    "        sampled_scenarios = []\n",
    "        sampled_ignition_points = []\n",
    "        sampled_ignition_dates = []\n",
    "        associated_fires = []\n",
    "        date_matched = []\n",
    "        distances = []\n",
    "        failed = 0\n",
    "        \n",
    "\n",
    "        #plt.scatter(filtered_data[\"LONGITUDE\"], filtered_data[\"LATITUDE\"])\n",
    "        #plt.axis(\"equal\")\n",
    "        #plt.show()\n",
    "\n",
    "        # start with the fires that have potewntial to be test fires, i.e their date is between the earliest and latest date\n",
    "        # We will have one \"test\" dataset, one \"train\" dataset, and one extra train dataset for the test fires\n",
    "\n",
    "        \n",
    "\n",
    "        for i, fire in filtered_data.iterrows():\n",
    "            # print the coordinates of the fire\n",
    "            width, height = fire['width'], fire['height']\n",
    "            # check that the scenario have the right size\n",
    "            if width != width_scenario or height != height_scenario:\n",
    "                print(f\"Scenario {first_scenario} has the wrong size for layout {layout_name}: {width} != {width_scenario} or {height} != {height_scenario}\")\n",
    "                failed_layouts.append(layout_name)\n",
    "                n_data = 0 \n",
    "                break\n",
    "            dataset = fire['dataset']\n",
    "            transformer = fire['transformer']\n",
    "            x_fire, y_fire = transformer.transform(fire['LONGITUDE'], fire['LATITUDE'], direction='INVERSE')\n",
    "            row, col = rasterio.transform.rowcol(dataset.transform, x_fire, y_fire)\n",
    "            date = fire['DISCOVERY_DATE']\n",
    "            # print(\"row, col\", row, col)\n",
    "            ignition_point = (row, col)\n",
    "            res = sampler.get_scenario_location(ignition_point, date = fire['DISCOVERY_DATE'].date(), leeway_distance=10, leeway_date=1, sampling_method='closest', exclude_scenarios=sampled_scenarios)\n",
    "            if res[0] is None:\n",
    "                sample, sampled_ignition_point, sampled_ignition_date = None, None, None\n",
    "            else:\n",
    "                sample, sampled_ignition_point, sampled_ignition_date = res\n",
    "            if sample is None:\n",
    "                failed += 1\n",
    "                print(\"we wanted to sample the fire\", fire['DISCOVERY_DATE'], \"at\", ignition_point, \"but it failed\")\n",
    "                #print(sampler.get_all_scenario_dates())\n",
    "                #print(sampler.get_all_scenario_ignition_points())\n",
    "                #print(\"-- surprising? --\")\n",
    "                #print(sampler.get_scenarios_at_a_date(fire['DISCOVERY_DATE'].date()))\n",
    "                continue\n",
    "            else:\n",
    "                print(sample)\n",
    "            sampled_scenarios.append(sample)\n",
    "            sampled_ignition_points.append(sampled_ignition_point)\n",
    "            sampled_ignition_dates.append(sampled_ignition_date)\n",
    "            associated_fires.append(fire['OBJECTID'])\n",
    "            distance = abs(sampled_ignition_point[0] - ignition_point[0]) + abs(sampled_ignition_point[1] - ignition_point[1])\n",
    "            distances.append(distance)\n",
    "        # plot the sampled scenarios\n",
    "        # the axes are inverted as coordinates start in (0,0) in the top left corner\n",
    "        #fig, ax = plt.subplots()\n",
    "        #ax.scatter([point[0] for point in sampled_ignition_points], [width - point[1] for point in sampled_ignition_points], color='red')\n",
    "        #plt.show()\n",
    "        #print(f\"Failed {failed}\")\n",
    "        # write the selected scenarios in a txt file\n",
    "        with open(f\"./WideDataset/{layout_name}/selected_scenarios_historical.txt\", \"w\") as f:\n",
    "            for scenario, fire_id in zip(sampled_scenarios, associated_fires):\n",
    "                f.write(f\"{scenario}, {fire_id}\\n\")\n",
    "            f.write(f\"Failed: {failed}\\n\")\n",
    "            failed_percentage = failed / max(n_data,1)\n",
    "            f.write(f\"Failed percentage: {failed_percentage}\\n\")\n",
    "            #print(f\"Layout {layout_name}: {n_data} fires, {failed} failed, {failed_percentage}\")\n",
    "            processed_layouts.append(layout_name)\n",
    "        if n_data != 0 and failed != n_data:\n",
    "            WORKED += 1\n",
    "            print(f\"WORKED: {WORKED}, {sampled_scenarios}\")\n",
    "            print(\"worked: \", layout_name)\n",
    "            plt.scatter(filtered_data[\"LONGITUDE\"], filtered_data[\"LATITUDE\"])\n",
    "            plt.axis(\"equal\")\n",
    "            plt.show()\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.scatter([point[0] for point in sampled_ignition_points], [width - point[1] for point in sampled_ignition_points], color='red')\n",
    "            plt.show()\n",
    "        if n_data != 0 and failed_percentage > 0.2:\n",
    "            print(f\"!! Failed {failed} out of {n_data} for layout {layout_name} !!\")\n",
    "            failed_layouts.append(layout_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error for layout {layout_name}: {e}\")\n",
    "        failed_layouts.append(layout_name)\n",
    "print(processed)\n",
    "print(total)\n",
    "print(processed_layouts)\n",
    "    \n",
    "\n",
    "# TODO do we need to create the grid manually? I think the raster file will doirectly give you the coordinates within the layout\n",
    "\n",
    "\n",
    "# for each fire, sample the scenario (space only, and time+space)\n",
    "# write the identifier in a txt file (one for space only, one for time+space)\n",
    "\n",
    "# move the scenarios into a selected folder\n",
    "# \"delete\" the other scenarios\n",
    "\n",
    "# train test split with the date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581759\n",
      "581759\n"
     ]
    }
   ],
   "source": [
    "print(len(new_fires_gdf['OBJECTID'].unique()))\n",
    "print(len(new_fires_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
